\documentclass[11pt,a4paper,titlepage,oneside]{report}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{float}
\usepackage{subfig}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}

\input{layout}

\setlength{\parindent}{0pt}

\title{ORB Slam Point Cloud generation on Apalis iMX8}
\author{Stefan Eichenberger}
\date{Januar 2019}
\advisors{Marcus Hudritsch}
\department{TSM CPVR Lab}

\lstset{
	basicstyle=\ttfamily\scriptsize
}

\begin{document}
\maketitle

\begin{abstract}
	For navigating a robot around obstacles we require a map of its environment. Because robots can be small and independent of power plugs, they have limited processing power. This work analyzes how suitable ORB SLAM is for an embedded processor and what kind of improvements would be possible.
\end{abstract}

\section*{Executive Summary}
Simultaneous Location and Mapping (SLAM) is an important topic in computer vision today. The possibilities for this technology are infinite. We can use SLAM for navigation, object recognition and augmented reality. Often this algorithms need a high performance graphics card or a high performance processor. For industrial and robotics purposes we have constraints in space, temperature and power consumption. This doesn’t allow the usage of a high performance CPUs. This work analyzes the ORB SLAM algorithm on how it performs on a modern embedded ARM CPU. The long-term goal is to have a system available that generates a map and to use this map for further navigation.

\tableofcontents

\chapter{Introduction}
To find the position of a robot we can use different sensors. Navigation systems use IMUs, magneto meters, radio signals and for outdoor navigation GPS. Humans however, do a lot of navigation by eye. Embedded systems today have enough processing power to make complex calculations. Therefore, a possibility to estimate the position of the robot can also be computer vision. One field in computer vision is SLAM where we try to create a map of the environment and to estimate the pose of the camera based on camera images. In this project, we analyze the performance of ORB SLAM \cite{orbslam} running on an iMX8QM as an embedded system. We use a stereo camera to create a point cloud and use this point cloud to calculate the changes in pose and position. As we will see, by using a stereo camera we can reconstruct a point cloud with a known scale. This allows us to create a 3D map with real world scale of the environment around the camera.

\section{Apalis iMX8QM}
Toradex supports this project with embedded hardware. Toradex is a system on module provider focusing on embedded ARM devices for industrial applications. They are interested to see some new applications and demos running on the new Apalis iMX8QM. Therefore, we try to port a SLAM application onto this platform. One area where NXP, the manufacturer of the processor, tries to place the iMX8 is computer vision in industry and automation. Therefore, this CPU should be a good fit for this kind of application. NXP launches different iMX8 variants. Publicly available today is only the iMX8M which is performance wise placed between the iMX8 and the iMX8X. The Apalis iMX8 Quad Max used in this project is the processor with highest performance. They will release it in 2019. This work uses an alpha silicon of the processor which is provided by Toradex.

\subsection{Features}

The iMX8 Quad Max has the following specification:
\begin{itemize}
  \item Two Cortex A72 high performance processors
  \item Four Cortex A53 low power processors
  \item Two Cotex M4 realtime MCUs
  \item Two Vivante GC7000 GPUs each with 8 cores that can calculated vec4 in float32
  \item Industrial temperature range from -40°C to 85°C
\end{itemize}

The iMX8 is focusing on industrial applications. Therefore, it doesn’t use state-of-the-art production processes. They produce the iMX8 in 28nm structure while Smartphone processors today use 7nm technology. However, bigger structures make the processor less prone to high temperatures. The two Cortex A72, the four Cortex A53 and the Vivante GPU have the performance level of a today's mid-end Smartphone. The two Cortex M4 processors could be used to do Realtime processing. However, we don’t use them in this work.

\section{Goals}
We can describe the goal of this project as follows:
\begin{itemize}
\item Porting ORB SLAM to the Apalis iMX8
\item Analyzing different possibilities to speed up the ORB SLAM algorithm
\item Add an addition to create a dense point clouds which could later be used to extract detailed maps
\item Evaluate the results and start discussion about further work
\end{itemize}

The focus of this work is to figure out how we can use or implement a SLAM algorithm that later could reconstruct the world around the camera in 3D. This 3D world or 3D point cloud could in a later step, that is not part of this work, used to plan paths around obstacles and to estimate the position of a camera in a known environment.

\section{Time Plan}

The project was planned to end on beginning of January. The concept phase started well. Unfortunately, standard ORB SLAM didn’t perform well for our purposes. First optimizations only improved the performance a little. However, we implemented a dense point cloud generator based on this implementation to figure out what challenges we have to fight there. The time plan of this project is shown in figure \ref{fig:timeplan}.
\begin{figure}[H]
	\includegraphics[width=1.0\textwidth]{img/timeplan.png}
	\caption{Time Plan}\label{fig:timeplan}
\end{figure}

\chapter{SLAM Evaluation}

In this section we analyze different documented SLAMs. We can split them into two groups indirect and direct SLAM see figure \ref{fig:slammodes}.Indirect methods analyze an image and try to extract features. This feature points are matched directly to further images. Based on these matches we can estimate the pose of the camera. Direct methods operate on intensity variances. The goal is to find a pose that minimizes the intensity difference between two images. Because minimizing the intensity difference over the whole image is computational expensive most methods operated only on edges or corners of the image. We call such methods direct semi dense or direct sparse. In the next sections we will have a discussion about these different approaches.

\begin{figure}[H]
	\includegraphics[width=1.0\textwidth]{img/slam_modes.png}
	\caption{SLAM Modes}\label{fig:slammodes}
\end{figure}


\section{Indirect method}

A well documented indirect method is ORB SLAM \cite{orbslam}. ORB SLAM extracts keypoints and calculates the ORB descriptors \cite{orb} around it, in further frames it again extracts ORB keypoints and matches them with the already known descriptors. Based on these matches it can do triangulation with RANSAC \cite{ransac} to estimate the current pose and position. Indirect methods have the advantage we can estimate the pose trough triangulation which isn’t computationally expensive. On the other side computing descriptors cost CPU time.

\section{Direct method}

In this section we will analyze two direct approaches a dense and a sparse approach. Dense methods use all pixels on the image for tracking while sparse methods only use a subset. Figure \ref{fig:sparse_dense} shows a comparison of dense sparse and semi-dense approaches. We don't dig into semi-dense approaches they work similar to sparse methods but with more keypoints.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=1.0\textwidth]{img/sparse_dense.png}
  \end{center}
	\caption{Comparison of sparse, dense and semi-dense approaches from paper \cite{svo}}\label{fig:sparse_dense}
\end{figure}

\subsection{Dense}

There are not that many dense SLAMs. One is DTAM \cite{dtam} which uses the intensity values of the whole image to estimating the pose. The idea is to minimize the intensity difference (energy) between two images by optimizing the camera pose. The energy defined at one pixel position is defined as shown in equation \ref{eq:pixel_energy}. To estimate the Pose it tries to find a P that minimizes $E_{t}$ as shown in equation \ref{eq:total_energy}.
\begin{equation}\label{eq:pixel_energy}
  E_{u}=I_1(u)-I_2(P*U)
\end{equation}
Where:
\begin{align*}
  E_{u}		&: \text{Energy at a specific position}\\
  I_{1/2}	&: \text{Image 1/2}\\
  u		&: \text{Position in image} \\
  P		&: \text{Pose of the camera} \\
  U		&: \text{Position of x in 3D}
\end{align*}
\begin{equation}\label{eq:total_energy}
  E_{t}=\min(\sum_{x=0}^X\sum_{y=0}^YE_u)
\end{equation}
Where:
\begin{align*}
  E_{t}		&: \text{Total energy}\\
  E_{u}		&: \text{Energy at a specific position}\\
	X				&: \text{Image columns}\\
	Y				&: \text{Image rows}\\
\end{align*}

One of the biggest advantages of using a direct dense method is that we receive a dense point cloud which we can use to create maps and 3d objects. A big disadvantage is however that they are computational expensive. Therefore, they are not appropriate for embedded devices.

\subsection{Sparse}

Sparse direct SLAMs like SVO \cite{svo} don’t optimize the Energy over the whole image but the energy at sparse points instead. One possibility of a sparse point is e.g. a corner point found by FAST \cite{fast}. Similar to sparse methods there are semi dense methods which try to minimize the energy with several points laying on edges. Canny Edge detection or DoG can for example find such edges. The advantage of sparse and semi dense methods is that they are less computationally expensive than dense methods.\\\\

Sparse direct methods don’t create dense clouds immediately however they can work on weaker keypoints than indirect methods. Therefore, they can create denser clouds than e.g. ORB SLAM. They can also be computational less expensive than indirect methods because they don’t have to calculate expensive features \cite{svo}.

\section{Stereo and Monocular SLAM}

In this project, the focus is on stereo SLAM. A lot of today's paper focus on monocular SLAM. The reasons are that first monocular cameras are cheaper and second monocular cameras are available in Smartphones. Monocular SLAM has two problems we don’t have with stereo SLAM. First an initialization process has to estimate the movement between two frames. We can only create a 3D point cloud based on two images with known position, in the monocular case the position of two images is random. Therefore, the algorithm needs to guess the pose and transformation over a few images until it knows the poses. It does that by making assumptions on the world or by trying to optimizing the point positions over several images. However, even when successfully initialized the second problem persists. Monocular SLAM is not able to guess the scale of the point cloud. The intuition for this issue is that we can’t distinguish between the movement of the camera on a small model close to the scene or the movement of the camera on the original model further away from the scene. With stereo SLAM we don’t have this issue. For each camera pose we get two images with a known distance between the two camera sensors (baseline). Therefore, we can calculate the depth of each pixel in real world distance. From this we can generate a point cloud assuming that the initial camera pose starts at (0,0,0). We therefore have an instantaneous initialization and can calculate the real world position in a known unit e.g. meters. Tracking between two camera poses is however the same on monocular and stereo SLAM. For stereo SLAM tracking mostly done only on one image. However we can again insert keypoints immediately on stereo cameras without the need of using a second camera pose. This makes stereo SLAM more robust than monocular SLAM.

\section{Decision}

In this project, we port ORB SLAM onto iMX8 and see how it performs. We used ORB SLAM because of the following reasons:
\begin{itemize}
	\item Open Source implementation available
	\item Powerful framework for displaying Point Cloud and images
	\item Good documentation
	\item Indirect methods seem promising because triangulation can be fast
	\item The CPVR lab already has experience with ORB on Smartphones
\end{itemize}

Based on ORB SLAM we try to analyze how we can use the iMX8 and whether ORB SLAM fits well for this platform.

\chapter{Camera Evaluation}

To do stereo computer vision, we need a stereo camera. There are a few stereo cameras available on the market. It is also possible to build a stereo camera with two monocular cameras. However, we decided against that because it would require a mechanism to synchronize the images from both streams. We analyze three stereo cameras available on the market.

\section{ZED}
The ZED camera \cite{zed} is a stereo camera from Stereo Labs. Here the specification of this camera:
\begin{itemize}
	\item Full HD color images with 30fps
	\item USB3.0
	\item UVC compliant
	\item Linux SDK
	\item Baseline 120mm
	\item IMU sensor with 6DoF
	\item Price: 449\$
\end{itemize}

To run their SDK a Dual Core CPU with 2.3 GHz and CUDA > 3.0 is required. However, it's also possible to use the camera without their SDK.

\section{ECON Tara}
The Tara camera \cite{tara} from Econ is another stereo camera. In comparison to the ZED camera it delivers a reduced resolution and only gray scale images.
\begin{itemize}
	\item 752x480 gray scale images with 60fps
	\item USB3.0
	\item Global Shutter Camera
	\item UVC compliant
	\item Baseline 60mm
	\item IMU sensor with 6DoF
	\item Open Source SDK
	\item Price: 149\$
\end{itemize}

It has a small OpenCV based SDK with open source software. In comparison to the ZED SDK it is less powerful.

\section{Intel RealSense D435}
The Intel RealSense camera \cite{realsense} is a infrared stereo camera. In comparison to RGB stereo camera it uses infrared images for stereo vision. It seems that this sensors are less prone for reflection. A third camera delivers RGB images in full HD.
\begin{itemize}
	\item 1280x720 with 90fps (IR), 1920x1080 with 30fps (RGB)
	\item USB3.0
	\item Global Shutter Camera
	\item UVC compilant (kernel patch required)
	\item Baseline 50mm
	\item Open Source SDK (librealsense)
	\item Price: 179\$
\end{itemize}

The Intel SDK seems quite powerful and is open source as well. However the indepth details of the camera are not publicly available.

\section{Decision}

We decided to use the Econ Tara because it has an outstanding price ratio and because it is a conventional stereo camera. However, the Intel RealSense camera is also great fit because it delivers depth images directly. This reduces the CPU load of the iMX8. If we require Full HD, we should use the ZED camera but keep in mind that for embedded systems Full HD processing may not be possible. Another advantage of the ZED camera is the bigger baseline of 120mm. This allows depth estimation with higher range. We decided against this camera because of the high price and because of the closed source SDK.

\chapter{Camera Calibration}

Camera calibration is necessary to find a model that expresses the properties of a camera. Properties of a camera are:

\begin{itemize}
	\item Focal length of the camera lense
	\item Principal point on the camera sensor, where the z axis of the cameras coordinate system goes through.
	\item Distortion of the camera lense
	\item Size of a pixel
\end{itemize}

One example where we need to know the camera model is if we project virtual objects into a real world image. Figure \ref{fig:model} shows an image of a checkerboard where we project a virtual cube onto. One camera model in this image took distortion into account while the other one ignored it.
\begin{figure}[H]
  \begin{center}
		\includegraphics[width=1.0\textwidth]{img/model.png}
  \end{center}
	\caption{Applied camera model with (red) and without(green) distortion}\label{fig:model}
\end{figure}

We also need to know the camera model when doing SLAM. With known camera model we can rectify (see \ref{fig:calibration_real}) the image so we can assume the camera as a ``perfect’’ pinhole camera model with known image center and known focal width. Additional to the above parameters we need to align the images of the left and right camera in stereo vision. The cameras aren’t perfectly aligned in the vertical direction however we need to have them aligned because this makes it possible to search for correspondence only on the epipolar line \cite{mvg}. Further the cameras are slightly rotated which means we must inverse rotate the image to have a perfect alignment. So additional to the properties of the camera we need to know the following parameters:
\begin{itemize}
	\item Rotation matrix of the right camera in regards to the left
	\item Y offset of the right and left camera in pixels to align the images perfectly
\end{itemize}

\section{Camera Model}

The camera model expresses how any point in the three-dimensional space is projected onto a two-dimensional image. As a first approximation it assumes that all rays are going trough one point. This is called the pinhole camera model \ref{fig:projection}a. Given this assumption we can describe the projection of a 3D point onto a 2D image as shown in equation \ref{eq:cm}. We calculate the pixel location $x,y$ on the image by normalizing with $s$ as show in equation \ref{eq:cm_normalized} \cite{rvc}.
\begin{equation}\label{eq:cm}
  \begin{pmatrix}
		f_x & \gamma & c_x \\
		0 & f_y & c_y \\
		0 & 0 & 1 \\
	\end{pmatrix}*
	\begin{pmatrix}
		r_{00} & r_{01} & r_{02} & t_x \\
		r_{10} & r_{11} & r_{12} & t_y \\
		r_{20} & r_{21} & r_{22} & t_z \\
	\end{pmatrix}
	\begin{pmatrix}
		X \\
		Y \\
		Z \\
		1
	\end{pmatrix}=
	\begin{pmatrix}
		u \\
		v \\
		s
  \end{pmatrix}
\end{equation}
\begin{equation}\label{eq:cm_normalized}
	\begin{pmatrix}
		x \\
		y
	\end{pmatrix}=
	\begin{pmatrix}
		u/s \\
		v/s 
  \end{pmatrix}
\end{equation}

Where:
\begin{align*}
  X,Y,Z			&: \text{point in the 3D world}\\
	u,v,s	   	&: \text{point in 2D image not normalize}\\
	x,y				&: \text{point in 2D image normalized with s}\\
	f_x,f_y  	&: \text{focal length of the camera}\\
  c_x,c_y  	&: \text{principal point}\\
  t_x,t_y,t_z	&: \text{location of the camera}\\
  r_{ij}	&: \text{part of the rotation matrix}
\end{align*}

We can describe the intuition as follows. A Point (X,Y,Z) is projected onto an image sensor (u/s,v/s) by the multiplication of the intrinsic times the extrinsic matrix. The extrinsic matrix describes where the pinhole of the camera is located in the three dimensional space. The intrinsic camera matrix describes how the camera is constructed. For example, in figure \ref{fig:projection}b we translate and rotate a point $p_i$ with the extrinsic matrix so we can describe its coordinates with the pinhole as origin. Further we transform the point with the intrinsic camera matrix onto the image sensor.

\begin{figure}[H]
	\centering
	\subfloat[Pinhole model]{\includegraphics[width=0.5\textwidth]{img/pinhole_camera_model.png}}
	\subfloat[3D point to 2D point]{\includegraphics[width=0.5\textwidth]{img/pnp.jpg}}
	\caption{Image projection}\label{fig:projection}
\end{figure}

The goal of camera calibration is to find the intrinsic camera matrix. With this camera matrix known we can calculate the position and pose of the camera by multiplying the projection matrix with the inverse of the intrinsic matrix as shown in equation \ref{eq:extrinsic}. A good reference for camera calibration is the OpenCV documentation \cite{opencv_calib}. We won't describe this process in more detail.

\begin{equation}\label{eq:extrinsic}
	E=I^{-1}*P
\end{equation}

\begin{align*}
	E		&: \text{Extrinsic Matrix}\\
	I		&: \text{Intrinsic Matrix}\\
	P		&: \text{Projection Matrix}
\end{align*}

\section{Stereo Camera Model}

Additional to the monocular parameters we need to find the parameter of the stereo camera. An image taken by a stereo camera normally is misaligned as shown in the top row of figure \ref{fig:stereo_calib}. The goal is to find the rotation matrix as well as the Y offset to get a perfectly aligned image pair as shown in the bottom row of figure \ref{fig:stereo_calib}.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/car_calib.jpg}
  \end{center}
	\caption{Stereo Calibration, top row misaligned image, bottom row rectified images}\label{fig:stereo_calib}
\end{figure}

\begin{figure}[H]
	\centering
	\subfloat[left image used for calibration]{\includegraphics[width=0.4\textwidth]{img/calib0_left.jpg}}
	\subfloat[right image used for calibration]{\includegraphics[width=0.4\textwidth]{img/calib0_right.jpg}}\\
	\subfloat[left image after rectification]{\includegraphics[width=0.4\textwidth]{img/calib0_left_rect.jpg}}
	\subfloat[right image after rectification]{\includegraphics[width=0.4\textwidth]{img/calib0_right_rect.jpg}}
	\caption{Top: Two images used to calibrate the camera, Bottom: New images after rectify}\label{fig:calibration_real}
\end{figure}

Figure \ref{fig:calibration_real} shows a ``real'' example of a calibrated stereo camera. In comparison to example \ref{fig:stereo_calib} this calibration also corrects distortion and equalizes fx and fy for both cameras.\\

Another parameter stereo calibration can calculate is the baseline between both cameras. The baseline is the distance of the centers of the left and the right camera. For that we have to specify the length of one checkerboard field. With known distance we can calculate the baseline. However, the calibration process will output focal length in pixels times baseline in meters as shown in equation \ref{eq:bf}. As we will see in equation \ref{eq:depth} this is the value needed for calculating the depth from disparity.

\begin{equation}\label{eq:bf}
	b_f=\frac{b*f}{p_s}
\end{equation}

\begin{align*}
	b_f &:	\text{Output of calibration process}\\
	b &:		\text{basline in m}\\
	f &:		\text{focal lenght in m}\\
	p_s &:	\text{Pixel size in m}\\
\end{align*}

\section{Checkerboard Size}

For camera calibration it is important to use a big checkerboard. The bigger the board the better the calibration result gets. The reason is hidden in equation \ref{eq:cm}. When we try to find the projection matrix we will always have a small error. This error comes from restrictions regarding resolution and misplacement while detecting corners. However, if the checkerboard is far away the error will be divided trough the distance because we are not interested in scale while doing camera calibration. This means the farer away the checkerboard is the smaller will be the influence of our error.\\
For calibrating the camera we can for example print a checkerboard on an advertisement board as e.g. offered by \cite{mydisplay}.\\
To see how well the calibration process performed we have to analyze the reprojection error that can be received by stereoCalibExtended of OpenCV. The error should be below 0.5 pixels, the smaller the better.\\
For calibrating the Econ Tara, a checkerboard of size $1500x1000mm$ was used. The Checkerboard fields have a size of $166x166mm$ which leads to $8x6$ inner corners detectable by the calibration script described in the next section.

\section{Econ Tara calibration}
For calibrating the Econ Tara camera we can use the econ\_calibration\_images.py scripts. It should be called as follows:
\begin{lstlisting}[language=bash]
python3 econ_calibration_images.py <calib folder> --size <field size>
\end{lstlisting}

<field size is the height and width of one checkerboard field. The images in the folder <calib folder> are gray scale and have the form <name><counter>\_left and <name><counter>\_right. We can use the script extract\_channels.py to create the images:
\begin{lstlisting}[language=bash]
python3 extract_channels.py <camera> <hidraw> <calib folder>
\end{lstlisting}
This script expects the path to the Econ Tara camera, the path to the hidraw device (to set auto-exposure) and the output folder.

\chapter{ORB SLAM}\label{chap:implementation}

In this chapter we discuss more deeply how ORB SLAM works and how we port it to the iMX8 processor.

\section{How it works}
ORB SLAM tries to find corners with FAST corner detection \cite{fast}. This gives us points which should lay on the intersection between two edges. We call this points keypoints. At this points we calculate the ORB descriptors \cite{orb} which describe the keypoints in a unique way. Based on this keypoints ORB SLAM will estimate the pose and position of the camera in various steps shown in figure \ref{fig:orb_slam2}.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/orb_slam2.png}
  \end{center}
	\caption{ORB SLAM2 \cite{orbslam2}}\label{fig:orb_slam2}
\end{figure}

ORB SLAM uses multiple threads to improve the performance of the algorithm, this threads and it's tasks are described in the following sections.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/pose_map.png}
  \end{center}
	\caption{ORB SLAM2 keyframes (blue), keypoints (red) current camera view (green)}\label{fig:pose_map}
\end{figure}

In the following section we will use the terms keypoints, keyframes, pose and position. In figure \ref{fig:pose_map} we see what this terms mean. The pose and position is where the image was taken and in which direction the camera was looking.

\subsection{Initialization}

Before starting with tracking the system needs an initialization step. In this step it generates the first Point Cloud at position $0,0,0$. For stereo SLAM this is step is simple:
\begin{enumerate}
	\item Calculate ORB keypoints on the left and right image
	\item Match the ORB descriptors of both images
	\item Calculate the disparity (difference) in pixel between the left and right image of each point
	\item Calculate depth with equation \ref{eq:depth}
	\item Calculate x and y position with equation \ref{eq:orb_pos}
\end{enumerate}	

\begin{equation}\label{eq:depth}
\begin{aligned}
	d&=\frac{f*b}{p_s*\Delta}\\
	\Delta&=x_{left}-x_{right}
\end{aligned}
\end{equation}
\begin{align*}
	d &:					\text{depth}\\
	b &:					\text{baseline in m}\\
	f &:					\text{Focal length in m}\\
	p_s	&:				\text{Pixel size in m}\\
	\Delta &:			\text{Difference between x position on left and rigth image in pixels}\\
	x_{left} &:	\text{X Position of keypoint on left image}\\
	x_{right} &: \text{X Position of keypoint on right image}
\end{align*}

\begin{equation}\label{eq:orb_pos}
\begin{aligned}
	X&=\frac{d*x}{f_x}\\
	Y&=\frac{d*y}{f_y}
\end{aligned}
\end{equation}
\begin{align*}
	X &:				\text{X position with camera as origin}\\
	Y &: 				\text{Y position with camera as origin}\\
	d &: 				\text{Depth=Z position with camera as origin}\\
	x,y &:			\text{x,y position in pixels in the image}\\
	f_x,f_y &:	\text{Focal length}
\end{align*}

This gives us an initial point cloud with camera as origin. We use this point cloud to estimate further poses as described in the following section. We will also have a look on the differences between stereo and monocular initialization in section \ref{sec:monster}.

\subsection{Tracking}

Tracking is the most important task where the algorithm estimates the pose and position of the current camera view. It performs the following steps:
\begin{enumerate}
	\item Find FAST corners
	\item Calculate ORB descriptors at FAST corners
\end{enumerate}

With known motion model from previous tracking:
\begin{enumerate}
	\item Search for matching ORB descriptors based on the motion model
	\item If the movement goes to the right we take the left image for the search if the movement goes to the left we take the right image
	\item Backproject points from 3D cloud with projection matrix found with motion model
	\item Optimize pose and position with Levenberg Marquardt \cite{levenbergmarquardt} to minimize the backprojection error
	\item Calculate the motion model by using the pose and position of the previous and current frame
\end{enumerate}

With unknown motion model:
\begin{enumerate}
	\item Search best matching keyframe with DBoW2 \cite{dbow}
	\item Match ORB descriptors with best fitting keyframe
	\item Triangulate the current pose as described in section \ref{sec:triangulation}
	\item Calculate the motion model by using the pose and position of the previous and current frame
\end{enumerate}

At the end we check if the current frame is a potential keyframe. We do this by checking how many frames were captured since the last keyframe, if a lot of outliers were detected and if more than 35\% of points are new with regards to the reference keyframe. If it fulfills everything the new frame is a possible keyframe.\\

\subsection{Local Mapping}
Local mapping tries to map the newly found keypoints into the world map. It only does local mapping for possible keyframes found in the previous step.
\begin{enumerate}
	\item Transform new keypoints to the global coordinate system based on the camera pose
	\item Do bundle adjustment \cite{bundleadjustment}. It is likely that points we already know do not 100\% match the position where we see the points. Bundle adjustment fuses the position of points found more than once.
	\item Check if more than 10\% of the points in the current frame compared to the points in all keyframes are new. If not we drop the keyframe.
\end{enumerate}

\subsection{Loop Closing}
Loop closing tries to detect loops and will then do a bundle adjustment to increase the precision of the point cloud.
\begin{enumerate}
	\item Match latest keyframe with all keyframes stored in the database with DBoW2 \cite{dbow}
	\item Optimize corresponding points by doing bundle adjustment over all key frame poses and point positions
\end{enumerate}

Loop Closing increases the precision of the point cloud because it can remove drift. However, it only takes effect if a loop is detected as shown in figure \ref{fig:loop_closing}.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/loop_closing.jpg}
  \end{center}
	\caption{A map before (top) and after (bottom) loop closing. The blue line shows the error before bundle adjustment.}\label{fig:loop_closing}
\end{figure}

\subsection{Triangulation}\label{sec:triangulation}

Triangulation is the step where we find a projection matrix based on a 3D/2D point mapping. We have to know at least 6 points for doing triangulation with the linear model. However, by using other models we could do triangulation with 3 points but we would have to use an non-linear model. By doing triangulation we want to find the projection matrix $p_{nn}$ in \ref{eq:projection}.

\begin{equation}\label{eq:projection}
	\begin{pmatrix}p_{11} & p_{12} & p_{13} & p_{14}\\
		p_{21} & p_{22} & p_{23} & p_{24}\\
		p_{31} & p_{32} & p_{33} & p_{34}\\
	\end{pmatrix}*
	\begin{pmatrix}
		X \\
		Y \\
		Z \\
		1
	\end{pmatrix}=
	\begin{pmatrix}
		u \\
		v \\
		s
  \end{pmatrix}
\end{equation}
Where:
\begin{align*}
	p_{ij}		&: \text{Unknown camera projection values of projection matrix P}\\
	X,Y,Z			&: \text{3D Point}\\
	u,v,s			&: \text{2D Point}\\
\end{align*}

We can rewrite equation \ref{eq:projection} as shown in equation \ref{eq:projection_flat}. We don't know u and v directly. We only have the pixel position which is $x=u/s, y=v/s, 1$. There is no way to estimate s. We know that s equals to the third row of equation \ref{eq:projection_flat}. If we multiply x and y by the third row, the equation will still hold (equation \ref{eq:projection_flat_s}). As a side effect we can remove the third row because the left and right side are equal which always holds. We finally have two equation per image point. Another trick we use is to set $p_{34}$ to 1. This makes the Z position of the camera the norm for the other unknowns. Because of this change the trivial solution where every unknown is zero disappears. We can solve the final equation \ref{eq:projection_flat_red} for all unknowns $p_{ij}$ with linear least square.

% Make sure we can have the whole column matrix (normally a line break will follow after 10 columns)
\setcounter{MaxMatrixCols}{15}
\begin{equation}\label{eq:projection_flat}
	\begin{pmatrix}
		X & Y & Z & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0 & X & Y & Z & 1 & 0 & 0 & 0 & 0\\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & X & Y & Z & 1\\
	\end{pmatrix}
	\begin{pmatrix}p_{00}\\
		p_{11}\\
		p_{12}\\
		p_{13}\\
		p_{14}\\
		p_{21}\\
		p_{22}\\
		p_{22}\\
		p_{23}\\
		p_{31}\\
		p_{32}\\
		p_{33}\\
		p_{34} \\
	\end{pmatrix}=
	\begin{pmatrix}u\\
		v\\
		s\\
	\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:projection_flat_s}
	\begin{pmatrix}u\\
		v\\
		s\\
	\end{pmatrix}=\begin{pmatrix}
		x*(X*p_{31}+Y*p_{32}+Z*p_{33}+p_{34})\\
		y*(X*p_{31}+Y*p_{32}+Z*p_{33}+p_{34})\\
		1*(X*p_{31}+Y*p_{32}+Z*p_{33}+p_{34})
	\end{pmatrix}
\end{equation}


\begin{equation}\label{eq:projection_flat_red}
	\begin{pmatrix}
		X & Y & Z & 1 & 0 & 0 & 0 & 0 & -xX & -xY & -xZ\\
		0 & 0 & 0 & 0 & X & Y & Z & 1 & -yX & -yY & -yZ
	\end{pmatrix}
	\begin{pmatrix}
		p'_{00}\\
		p'_{11}\\
		p'_{12}\\
		p'_{13}\\
		p'_{14}\\
		p'_{21}\\
		p'_{22}\\
		p'_{23}\\
		p'_{24}\\
		p'_{31}\\
		p'_{32}\\
		p'_{33}
	\end{pmatrix}=
	\begin{pmatrix}x\\
		y\\
	\end{pmatrix}
\end{equation}
Where:
\begin{align*}
	p_{ij}		&: \text{unknown camera projection values}\\
	p'_{ij}		&: \text{same as c but normalized to $p_{34}$}\\
	X,Y,Z			&: \text{3D Point}\\
	u,v,s			&: \text{2D Point with scale factor s}\\
	x,y				&: \text{2D Point normalize to s=1}\\
\end{align*}

We miss one last step. We set $p_{34}$ to 1 to get rid of the trivial solution where all $p_{ij}$ are zero. We now have to find the right value for $p_{34}$. If we calculate the intrinsic times the extrinsic matrix we end up in equation \ref{eq:ext_int}. We focus on the third row. We know that the norm of a column of a rotation matrix must be one. The reason for this is that a rotation matrix will never change the length of a vector. Therefore, $h=\sqrt{r_{31}^2+r_{32}^2+r_{33}^2}$ must be one \cite{Wu}. Because we normalized tz to one, this assumption does not hold for our solution. The camera distance $t_z$ will therefore be $1*1/h$. To correct the projection matrix we can multiply the matrix with $1/h$ (equation \ref{eq:ext_int_scaled}).

\begin{equation}\label{eq:ext_int}
	P=
	\begin{pmatrix}
		f_xr_{11}+p_xr_{31} & f_xr_{12}+p_xr_{32} & f_xr_{13}+p_xr_{33} & f_xt_x+p_xt_z\\
		f_yr_{21}+p_yr_{31} & f_yr_{22}+p_yr_{32} & f_yr_{23}+p_yr_{33} & f_yt_y+p_yt_z\\
		r_{31} & r_{32} & r_{33} & t_z
	\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:ext_int_scaled}
	P=1/h
	\begin{pmatrix}
		p'_{11} & p'_{12} & p'_{13} & p'_{14}\\
		p'_{21} & p'_{22} & p'_{23} & p'_{24}\\
		p'_{31} & p'_{32} & p'_{33} & 1\\
	\end{pmatrix}
\end{equation}

Where:
\begin{align*}
	P					&: \text{projection matrix}\\
	p_{ij}'		&: \text{projection matrix found with $t_z=1$}\\
	f_x,f_y		&: \text{focal length}\\
	c_x,c_y		&: \text{principal point}\\
	r_{ij}		&: \text{Parameters of rotation matrix}\\
	t_{x,y,z}	&: \text{location of the camera}\\
	h					&: \text{scaling inverse $tz=1/\sqrt{r_{31}^{'2}+r_{32}^{'2}+r_{33}^{'2}}$}
\end{align*}


Now that we know all unknowns we can calculate the extrinsic matrix that describes the pose. We need to multiply the inverse intrinsic matrix times the projection matrix as shown in equation \ref{eq:extrinsic}. If we prefere angles instead of the rotation matrix we can extract them as shown in equation \ref{eq:angles}. Equation \ref{eq:angles_proof} shows the rotation matrix derived from angles. To get back angles we use the fact that $arctan(sin(x)/cos(x))=x$.
\begin{equation}\label{eq:angles_proof}
\begin{aligned}
	&R=\begin{pmatrix}
		&c(\theta_y)c(\theta_z) & -c(\theta_y)s(\theta_z) & s(\theta_y)\\
		&c(\theta_x)s(\theta_z)s(\theta_y)c(\theta_z) & c(\theta_x)c(\theta_z)-s(\theta_x)s(\theta_y)s(\theta_z) & -s(\theta_x)c(\theta_y)\\
		&s(\theta_x)s(\theta_z)-c(\theta_y)c(\theta_z) & c(\theta_x)s(\theta_y)s(\theta_z)+s(\theta_x)c(\theta_z) &	c(\theta_x)c(\theta_y)\\
	\end{pmatrix}
\end{aligned}
\end{equation}
Where:
\begin{align*}
	\theta_n	&: \text{Rotation in n direction}\\
	s				  &: \text{sin}\\
	c				  &: \text{cos}\\
\end{align*}

From that we get:
\begin{equation}\label{eq:angles}
\begin{aligned}
	\theta_y &= arcsin(sin(\theta_y)) = arcsin(r_{02})\\
	\theta_x &= arctan(\frac{-sin(\theta_x)cos(\theta_y)}{cos(\theta_x)cos(\theta_y)}) = arctan(\frac{r_{12}}{r_{22}})\\
	\theta_z &= arctan(\frac{-cos(\theta_y)sin(\theta_z)}{cos(\theta_y)cos(\theta_z)}) = arctan(\frac{r_{01}}{r_{00}}) 
\end{aligned}
\end{equation}
Where:
\begin{align*}
	\theta_n	&: \text{Rotation in n direction}\\
	r_{nn}	  &: \text{Rotation matrix term see equation \ref{eq:cm}}\\
\end{align*}

Because we normally have more than 6 points available for triangulation we use RANSAC \cite{ransac} to find a good solution and to eliminate outliers.

\section{Comparison Mono to Stereo}\label{sec:monster}

In this work we only talk about stereo SLAM. But what are the differences to a monocular SLAM system?\\
A monocular camera can't see depth. This means that from one single image we can't estimate the keypoints in the 3D world. Therefore, we need an initialization process that will try to estimate the depth over several images. There are different strategies for solving the initialization problem. ORB SLAM calculates two models in parallel one when most points are laying on a plane and one model when most points are randomly distributed. In one model we search the essential matrix (fundamental matrix with 5 DoF) in the other we search the homography matrix \cite{orbslam}. If the initialization succeeds one of this matrices will converge to a small reprojection error while the other wont. The camera pose and position is calculated by decomposing the converged matrix.\\
Other approaches use an extended Kalman filter (EKF) and try to optimize the camera pose over the first n frames \cite{svo}. The EKF starts with random depth values and converges after several frames to the ``real'' depth values. For a monocular camera it is not possible to find all 6 DoF because the fundamental and homography matrix only have 5 DoF. They therefore set the 6 parameter to 1 which means we don't get a real ``scale'' value. All measurements with monocular cameras are therefore scales.

\section{Port to iMX8}\label{sec:orbport}

The iMX8 BSP is built with Yocto. NXP and Toradex provide a BSP layer which contains a kernel and all necessary proprietary libraries for OpenGL. OpenCV is already part of the image fsl-image-qt5. How we build the BSP and the SDK is out of scope of this project. Please check the following references:\\
\begin{itemize}
	\item Yocto project \cite{yocto}
	\item Toradex BSP \cite{toradex_bsp}
\end{itemize}

From the Yocto build we can also get an SDK which contains a ARM64 toolchain that can be used to compile additional programs and libraries. This toolchain was used to build ORB SLAM2 for iMX8. For the following section we assume that the toolchain is installed under /opt/fsl-imx-x11/4.9.51-mx8-beta.

\subsection{Pangolin}
Pangolin \cite{pangolin} is required by ORB SLAM for showing the sparse map and the camera pose. A simple git checkout of the pangolin sources is enough to receive the sources. Before building the project with CMake the following patch should be applied:
\begin{lstlisting}
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 32a2d78..683ef38 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -15,6 +15,9 @@ SET(CPACK_DEBIAN_PACKAGE_MAINTAINER "Steven Lovegrove")
 SET(CPACK_PACKAGE_VERSION_MAJOR ${PANGOLIN_VERSION_MAJOR})
 SET(CPACK_PACKAGE_VERSION_MINOR ${PANGOLIN_VERSION_MINOR})
 SET(CPACK_PACKAGE_VERSION_PATCH "0")
+SET(CMAKE_INCLUDE_SYSTEM_FLAG_CXX "-I")
+SET(CMAKE_INCLUDE_SYSTEM_FLAG_C "-I")
+
 include(CPack)
 
 option( BUILD_EXAMPLES "Build Examples" ON )
\end{lstlisting}

After that the following commands will compile and install Pangolin:
\begin{lstlisting}[language=bash]
. /opt/fsl-imx-x11/4.9.51-mx8-beta/environment-setup-aarch64-poky-linux
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX=<INSTALL_DIR> ..
make -j
make install
\end{lstlisting}

This will build the Pangolin library for ARM64. INSTALL\_DIR should point to a directory which is used as local ``root filesystem''. This root filesystem should be copied via scp to the iMX8.

\subsection{OpenCV}\label{sec:opencv}
We modified ORB SLAM 2 for this project by adding different ORB implementations and by adding Densification. To make this version work we need the modified OpenCV version from \cite{opencv_se}. The branch se-3.4.3 contains some modification which allows to reuse the image pyramid generated by ORB. Also the CMake flags are changed to allow compilation on iMX8. Additionally we need to compile OpenCV Contribute \cite{opencv_contrib} version tag 3.4.3. To build OpenCV plus OpenCV Contribute the following commands are required:
\begin{lstlisting}[language=bash]
. /opt/fsl-imx-x11/4.9.51-mx8-beta/environment-setup-aarch64-poky-linux
. /opt/fsl-imx-x11/4.9.51-mx8-beta/sysroots/x86_64-pokysdk-linux/environment-setup.d/cmake.sh

mkdir build
cd build

export PKG_CONFIG_LIBDIR=/opt/fsl-imx-x11/4.9.51-mx8-beta/sysroots/aarch64-poky-linux/usr/lib/pkgconfig/
cmake -DWITH_ITT=NO -DWITH_LIBV4L=YES -DWITH_TBB=NO -DOPENCV_EXTRA_MODULES_PATH=<path to OpenCV contrib> -DCMAKE_BUILD_TYPE=Release ..
\end{lstlisting}

The libraries found in the lib directory we have to copy to /usr/lib on the iMX8.

\subsection{ORB SLAM 2}
The sources of this project are based on ORB SLAM 2 \cite{orbslam2_impl}. Instead of using the sources of the original the modified sources \cite{orbslam2_se} under branch work\_se should be used. It contains some fixes for newer compiler versions as well as additional sources for the Econ Tara camera. After fetching the sources with git the following commands will build the binary:\\
\begin{lstlisting}[language=bash]
. /opt/fsl-imx-x11/4.9.51-mx8-beta/environment-setup-aarch64-poky-linux
mkdir build
cd build
cmake -DPangolin_DIR=<INSTALL_DIR/usr/lib/cmake/Pangolin> -DCMAKE_INSTALL_PREFIX=<INSTALL_DIR> -DOpenCV_DIR:PATH=<OPENCV_BUILD_DIR> ..
make -j
make install
\end{lstlisting}

This will build ORB SLAM for ARM64. INSTALL\_DIR is the same directory as it was for Pangolin. After that we can copy the whole INSTALL\_DIR to the iMX8 root directory and start econ\_stereo to start the SLAM system. The OPENCV\_BUILD\_DIR should point to the build directory of OpenCV in section \ref{sec:opencv}.

\subsection{ORB SLAM 2 usage}

For the Econ Tara a special stereo\_econ application is available. This application can be called as follows:
\begin{lstlisting}[language=bash]
./stereo_econ path_to_vocabulary path_to_settings path_to_video
\end{lstlisting}

The first argument path\_to\_vocabulary points to the ORB dictionary for DBoW2, path\_to\_settings should point to the Econ.yaml and path\_to\_video points to the Econ Stereo Camera device.\\
In comparison to the orignal ORB SLAM 2 implementation some additional parameters are added:\\
\begin{tabular}{l l}
stereosgbm.cn & CN parameter fo sgbm \\
stereosgbm.preFilterCap & Pre filter capacity for sgbm \\
stereosgbm.windowSize & Window size for sgbm \\
stereosgbm.blockSize & Block size for sgbm \\
stereosgbm.minDisparity & Minimum disparity for sgbm \\
stereosgbm.speckleRange &  Speckle Range for sgbm \\
stereosgbm.disp12MaxDiff & Max diff for sgbm \\
stereosgbm.uniquenessRatio & Uniqueness ratio for sgbm \\
stereosgbm.speckleWindowSize & Speckle window size for sgbm \\
stereosgbm.numberOfDisparities & Total number of disparities for sgbm \\
cpu.system & CPU assigned to system thread \\
cpu.stereoleft & CPU assigned to orb thread for left image \\
cpu.stereoright & CPU assigned to orb thread for right image \\
cpu.loopclosing & CPU assigned to loop closing thread \\
cpu.localmapping & CPU assigned to local mapping thread \\
cpu.densify & CPU assigned to densify thread \\
cpu.viewer & CPU assigned to viewer \\
Densify.enabled & 0 if densify is disabled 1 if enabled \\
ORBextractor.orbextractor & Orb extractor: 0->Original, 1->OpenCV, 2->OpenCV OCL \\
Densify.topmargin & top margin for depth image \\
Densify.bottommargin & bottom margin for depth image \\
Densify.leftmargin & left margin for depth image \\
Densify.rightmargin & right margin for depth image \\
\end{tabular}\\

The SGBM features are hard to tune the values in Econ.yaml were taken from the stereo\_match.cpp example of OpenCV \cite{opencv_se}. The ORBextractor.orbextractor can be used to switch between different ORB implementations in section \ref{sec:results}. Densify.*margin is used to remove some margin from depth images. This is necessary because points close to the boarders have high uncertainty.

\chapter{Densification}

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/dense_cloud.png}
  \end{center}
	\caption{A dense cloud generated with SGBM and ORB SLAM}\label{fig:dense_cloud}
\end{figure}

ORB SLAM only delivers a very sparse map. Therefore, it can't be used e.g. for creating a map or to reconstruct 3D objects. It's focus lays on delivering a stable pose of the camera. In this section we discuss how we can improve ORB SLAM by densifing the point cloud as shown in figure \ref{fig:dense_cloud}. We also show a first implementation which however isn't state of the art as we will see later.\\
In the first section we will see how the stereo images can be used to create a depth map. The next section describes how to use the depth map to generate a point cloud. The two last sections discuss two possibilities on how a depth map of several poses could be fused together. However, as we will see the current approach doesn't work as expected because the point cloud size increases too fast. Therefore, we don't have CPU time to fuse the point cloud effectively.

\section{Depth Map}

A depth map is an image that contains depth values instead of color intensities. They are generated from stereo cameras or from ToF cameras. We will only talk about how depth images are made from stereo cameras. Image \ref{fig:depth} shows a depth image made from two stereo images.

\begin{figure}[H]
	\centering
	\subfloat[left image]{\includegraphics[height=100px]{img/disparity_left.png}}
	\subfloat[right image]{\includegraphics[height=100px]{img/disparity_right.png}}
	\subfloat[depth image]{\includegraphics[height=100px]{img/disparity.png}}
	\caption{Depth image from stereo}\label{fig:depth}
\end{figure}

As we see in figure \ref{fig:depth} the depth image is noisy in areas where we have low or no texture. The algorithm does an intensity block matching. Because we vertically aligned the camera, the algorithm starts matching at the pixel position of the left image and searches from there to the right as shown in figure \ref{fig:disparity}. The minimal intensity is the best match and therefore the postion we take. We call the distance between the left and right image disparity. An object far away has less disparity than a near object. A point at infinity will have a disparity of zero.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/disparity_concept.png}
  \end{center}
	\caption{Disparity with epipolar (horizontal) line, left: left image, right: right image with transparent left image}\label{fig:disparity}
\end{figure}

As described we calculate the disparity by doing block matching. However, this algorithm is prone to noise therefore a we use a modified version of this algorithm called Semi Global Block Matching (SGBM) \cite{sgbm}. Compared to normal Block Matching this algorithm penalties big disparity differences between neighbouring pixels. We also calculate the disparity for the left and right image so we get values for pixels that are only visible in one image too. Further we can generate a confidence map if pixels have different disparities when we do left and right matching. We can use this confidence map to filter vague disparities and replace them with values of their neighbourhood.

\section{Point Cloud from Depth}

From the depth map generated by the previous section we calculate a dense point cloud based on the current pose. We transform the depth information into the three dimensional space. An example of such a point cloud is shown in figure \ref{fig:pointcloud}. The point cloud is generated from the depth image by transforming each pixel with the current camera pose.

\begin{equation}\label{eq:point_cloud_depth}
	\begin{pmatrix}
			X \\
			Y \\
			Z
	\end{pmatrix}=
	\begin{pmatrix}
		r_{00} & r_{01} & r_{02} & t_x \\
		r_{10} & r_{11} & r_{12} & t_y \\
		r_{20} & r_{21} & r_{22} & t_z \\
	\end{pmatrix}*
	\begin{pmatrix}
		x,
		y,
		d,
		1
	\end{pmatrix}
\end{equation}
\begin{align*}
	X,Y,Z &:			\text{World position of the pixel}\\
	r_{ij} &:			\text{Rotation elements of pose}\\
	t_{x,y,z} &:	\text{Translation elements of pose}\\
	x,y:					\text{u,v translated to world, see \ref{eq:orb_pos}}\\
	d &:					\text{depth}
\end{align*}

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/pointcloud.png}
  \end{center}
	\caption{Dense point cloud generated from different key frames}\label{fig:pointcloud}
\end{figure}

To reduce the CPU load, we add new dense points to the cloud only for keyframes. What we see in figure \ref{fig:pointcloud} is that because of the uncertainty of the depth map we have tons of outliers. Further we don’t fuse points at the same location therefore we generate a lot of points that have the same position within a statistical uncertainty.

\section{Point Cloud fusion}

There are different papers that describe the problem of point cloud fusion. If we have two views that partially show the same region we have to merge the overlapping part. Here we see two possibilities of how we could fuse two point cloud. REMODE is a probabilistic approach while volumetric 3D mapping is a more direct approach.

\subsection{REMODE}

REMODE uses a probabilistic approach to fuse data from different views. Only points with high enough probability are drawn on the 3D map. This generates a semi-dense point cloud which has a high probability of being true. However, it requires a lot of frames seeing the same scene for getting high accuracy, this is exactly what we try to avoid because we already have problems to perform well with our current approach. Therefore, this algorithm isn’t suitable for us.

\subsection{Volumetric 3D Mapping}

The approach of using a volumetric 3D mapping is interesting. The algorithm is designed to run on a CPU only. It assigns a box to each 3D pixel and puts this box into an octree. If we want to add a new pixel, the algorithm checks whether it fits in an existing box. Unfortunately, a short test implementation showed that even with this approach the CPU load explodes when using images at full resolution. Therefore, we would have to consider a different approach for a final solution which unfortunately didn’t fit into this time window.

\chapter{Discussion}

In this chapter we discuss the current status of this work. We analyse the achievements and see if we can improve the approach to achieve realtime location and mapping.

\section{Results}\label{sec:results}

The current implementation is heavily based on the original ORB SLAM 2 implementation as described in section \ref{sec:orbport}. Because we used normal ORB SLAM the results from the original paper are still valid. However, what we are interested in is the realtime frame rate and therefore the time used per frame. The original ORB SLAM 2 implementation calculates this time per frame and prints the mean and median time after each run. For comparison we used the KITTI04 dataset with 271 images. The resolution of each image is 1226x370 where the ECON Tara has a resolution of 752x480. The KITTI dataset is a standard benchmark for SLAM therefore it was taken. In comparison to Euroc another dataset it has smaller sequences and finishes therefore faster.\\\

We compare the ORB SLAM ORB implementation with the ORB implementation from OpenCV. To be fair we reduce the feature count when using OpenCV from 1000 to 800 because OpenCV ORB finds more features than the ORB SLAM implementation. The bigger the point cloud gets the slower the algorithm gets, therefore it wouldn't be a fair comparison.

\begin{table}
	\begin{tabular}{  | l | l | l | l | l | l | }
		\hline
		\textbf{CPU} & \textbf{Impl.} & \textbf{feat.} & \textbf{feat. det.} & \textbf{median tt} & \textbf{mean tt} \\ \hline
		iMX8 & ORB & 1000 &  548 & 0.18307 & 0.185202 \\ \hline
		iMX8 & ORB, Linux scheduled & 1000 & 548 & 0.390867 & 0.387753 \\ \hline
		iMX8 & OpenCV & 1000 & 757 & 0.190571 & 0.210072 \\ \hline
		iMX8 & OpenCV & 800 & 610 & \textbf{0.155441} & \textbf{0.167176} \\ \hline
		iMX8 & OpenCV, Linux scheduled & 800 & 610 & 0.358689 & 0.362126 \\ \hline
		iMX8 & OpenCV OpenCL & 800 & 610 & 0.50622 & 0.771859 \\ \hline
		i5-7Y54 & ORB & 1000 & 546 & 0.18884 & 0.184628 \\ \hline
		i5-7Y54 & OpenCV & 1000 & 756 & 0.154901 & 0.152786 \\ \hline
		i5-7Y54 & OpenCV & 800 & 610 & 0.135715 & 0.132304 \\ \hline
	\end{tabular}
	\caption{Comparison of ORB SLAM tracking time in KITTI04 using ORB or OpenCV implementation}
  \label{tab:result}
\end{table}

As we see from table \ref{tab:result} we achieve best timings on the iMX8 when using the OpenCV ORB SLAM implementation. We could extend this version so it behaves the same as the ORB SLAM implementation and would still enjoy the improvement. However, we are still at a low frame rate of around 6 frames per seconds. When we compare the result to a i5-7Y54 Intel processor, we achieve similar results at around 7fps. Unfortunately, none of these result is near to 20fps. We consider 20fps as realtime because our eye would recognize that as fluent. 6-7fps work well for slow movements but the system can’t handle fast movements because of the bad performance. The biggest disappointment is that the OpenCL implementation of ORB in OpenCV is worse than the CPU implementation. We therefore can’t increase the performance by using the GPU instead of the CPU. Even if we tweak the OpenCL implementation, we wouldn’t get close to the implementation on the CPU.

\section{Optimizations}

The current frame rate on the iMX8 as well as on a laptop CPU is too low. This makes it hard to get a robust tracking and mapping.

\subsection{Different ORB Implementation}
Replacing the ORB algorithm from ORB SLAM with the one from OpenCV we see that higher frame rates are possible. However, this improves it only by around 20\% and we don’t profit from the better keypoint distribution of ORB SLAM. We could add the better distribution to OpenCV ORB by changing the OpenCV implementation

\subsection{Using OpenCL}
OpenCV ORB supports finding FAST corners and calculating ORB descriptor with OpenCL. However, a first test showed that using OpenCL is even slower than using the CPU. The reason for this is probably that the overhead is too high. It could be that a different algorithm that is designed to run completely on GPU would benefit much more.

\subsection{Reduce Resolution}
Using lower resolution images would improve the performance of the algorithm. However, this is not something we want to do because it also decreases the stability. With a resolution of 752x480 we already limit the resolution to a realistic level for embedded devices.

\section{Problems}

\subsection{Big Algorithm}
ORB SLAM is a powerful algorithm. The sources are also a bit obfuscated in how it works. The algorithm profits from all parts shown in figure \ref{fig:orb_slam2}. Therefore, it's hard to remove only parts of it. A lot of dependencies to modified 3rd party libraries like g2o or libeigen makes it hard to do further optimization on the algorithm.

\subsection{Descriptor Calculation}
ORB descriptors are stable over different images and can handle different illumination up to a certain level. However, it seems that calculating the descriptors already consumes quite some CPU time as we saw in section \ref{sec:results}.

\subsection{Newer approaches}
A lot of newer approaches for SLAM nowadays talk about direct approaches. The overhead of direct matching is less problematic than calculating descriptors. If we achieve high frame rates, it limits the search radius which reduces the CPU load. By using IMU to track the movement we could limit the search radius even more. This is unfortunately not fully true for ORB SLAM because we always have to calculate the ORB descriptors. Therefore, the next chapter should summarize what approach we should investigate to get a fast and robust SLAM.

\chapter{Direct Approach}
Because a lot of new papers describe sparse direct approaches as faster than indirect methods we analyze one of them here. This could be the basis for further work specially when using it in combination with data from an IMU.

\section{SVO}

SVO stands for sparse visual odometry. It is a sparse direct approach to the SLAM problem. The first monocular implementation was released as open source however the improved SVO2 source code is not publicly available. However, the paper describes the algorithm in good enough detail so it should be possible to do a similar implementation. We can describe the main parts of the algorithm as follows.

Initialization (Stereo):
\begin{enumerate}
	\item Search corner points with FAST
	\item Split the image into areas of fixed size (e.g. 32x32)
	\item Take the FAST corner with maximum response in each area if there is any
	\item (optional) If there is no FAST corner take the point with highest gradient on an edge (see \ref{fig:svo})
	\item With a stereo camera we can do a template matching for each of this ~100 points to find the depth of each point
	\item Add current frame as key frame
\end{enumerate}

Tracking:
\begin{enumerate}
	\item For each point we try to minimize the photometric error (intensity difference)
	\item We use a motion model received from previous tracking to optimize the pose and position with Levenberg-Marquardt
		\subitem (optional) The motion model could be improved by IMU
	\item (optional) For each point we calculate the backprojection into the image
	\item (optional) For each point we search the minimal intensity difference withing a small window around the backprojection
	\item (optional) We calculate the difference of the position from backprojection and intensity search
	\item (optional) We adjust the 3D points with bundle adjustment to minimize the missmatch of the previous step
\end{enumerate}

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=1.0\textwidth]{img/svo.png}
  \end{center}
	\caption{Example of sparse points used by SVO in Euroc dataset \cite{svo}. The green points are fast corners the magenta points are edgelets used in areas with no corners}\label{fig:svo}
\end{figure}

This algorithm can't do loop closing as ORB SLAM does. However, it would be possible to add such improvement if necessary (e.g. by using orb descriptors only on reference frames). Due to it's simplicity they report 13.27ms to process one 640x480 frame in EUROC Machine Hall 1 on an Nvidia Jetson TX1. The CPU of this processor is comparable to the one on the iMX8. This would end up in a frame rate up to 75fps which is above 60fps, the maximum of the Econ camera.

\section{Densification}

In the SVO paper they describe the possibility to do densification based on sparse points. The problem however is that dense clouds consume a lot of memory, which is not something you have granted on embedded systems. Because SVO uses corners as key points an approach could be to generate a mesh between all keypoints. To get a better feeling of the environment, keyframes can serve as textures to render 3D maps. To increase the resolution we need to go closer to the object, with a closer view we will detect more keypoints in the same area which makes the cloud denser. With this approach we wouldn’t require a point for each pixel but only a point for keypoints. This would leave us room to do filter operations (like plane detection) on the 3D cloud on a reduced number of points.

\section{Outlook}
SVO SLAM is especially interesting because of its simplicity and because it is possible to remove a lot of optional features. By removing this optional features the tracking results are getting worse but if we combine tracking with data from the IMU, we can hopefully compensate the loss of precision. Together with a stereo camera we also don’t need the complicated initialization process described in the SVO paper because we can directly calculate the depth of each point with the first frame.

\chapter{Conclusion}
Unfortunately, the current result for ORB SLAM shows it is not convenient for embedded devices. It is hard to optimize the algorithm because of its complexity. Even with multi-threading and multiprocessing in place we can’t speed up the algorithm to over 10 frames per second. Therefore, we propose to analyze a direct sparse approach in further work for doing SLAM instead of using an indirect sparse approach. The disadvantage of being computationally more expensive in pose estimation is compensated by the advantage of not needing to extract expensive features. We showed that we can reduce SVO to a simple implementation when not using all optional features. We therefore propose to use SVO in further work for doing SLAM and to generate a mesh from keypoints found by SVO. We will be able to use this mesh to extract maps and to reconstruct 3D objects.

\listoffigures
 
\begin{thebibliography}{1}

	\bibitem{orb}
	E. Rublee, V. Rabaud, K. Konolige, and G. Bradski\\
	\textit{ORB: an efficient alternative to SIFT or SURF}\\
	IEEE International Conference on Computer Vision (ICCV), Barcelona, Spain, November 2011, pp. 2564–2571.

  \bibitem{orbslam}
  Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos\\
  \textit{ORB-SLAM: a Versatile and Accurate Monocular SLAM System}\\
  arXiv:1502.00956v2

  \bibitem{orbslam2}
  Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos\\
  \textit{ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras}\\
	arXiv:1610.06475v2 

	\bibitem{dtam}
	Richard A. Newcombe, Steven J. Lovegrove and Andrew J. Davis\\
	\textit{DTAM: Dense Tracking and Mapping in Real-Time}\\
	doi:10.1109/ICCV.2011.6126513

	\bibitem{svo}
	Christian Forster et al\\
	\textit{SVO: Semi-Direct Visual Odometry for Monocular and Multi-Camera Systems}\\
	doi:10.1109/TRO.2016.2623335

	\bibitem{fast}
	E. Rosten, R. Porter, and T. Drummond \\
	\textit{Faster and better: A machine learning approach to corner detection}\\
	doi:10.1109/TPAMI.2008.275

	\bibitem{mvg}
	Richard Hartley\\
	\textit{Multiple View Geometry in Computer Vision}\\
	ISBN-10: 0-511-18618-5, p240-249

	\bibitem{levenbergmarquardt}
	Wikipedia\\
	\textit{Levenberger-Marquardt algorithm}\\
	https://en.wikipedia.org/wiki/Levenberg\%E2\%80\%93Marquardt\_algorithm

	\bibitem{zed}
	Stereo Labs\\
	\textit{ZED Stereo Camera}\\
	https://www.stereolabs.com/zed/

	\bibitem{tara}
	Econ\\
	\textit{Tara Stereo Camera}\\
	https://www.e-consystems.com/3D-USB-Stereokamera-de.asp

	\bibitem{realsense}
	Intel\\
	\textit{Intel RealSense}\\
	https://realsense.intel.com/depth-camera/\#D415\_D435


	\bibitem{opencv_calib}
	OpenCV\\
	\textit{Camera Calibration}\\
	https://docs.opencv.org/3.4.3/dc/dbb/tutorial\_py\_calibration.html

	\bibitem{mydisplay}
	mydisplay.ch\\
	\textit{Advertisement board manufacturer}\\
	https://www.mydisplays.ch/

	\bibitem{yocto}
	Yocto Project\\
	\textit{Yocto Project Mega Manual}\\
	https://www.yoctoproject.org/docs/latest/mega-manual/mega-manual.html

	\bibitem{toradex_bsp}
	Toradex AG\\
	\textit{Build Apalis iMX8 OpenEmbedded Project Bring-up Image}\\
	https://developer.toradex.com/software/linux/linux-software/build-apalis-imx8-yoctoopenembedded-bring-up-image

	\bibitem{dbow}
	D. Gálvez-López and J. D. Tardós\\
	\textit{Bags of binary words for fast place recognition in image sequences}\\
	IEEE Trans. Robot., vol. 28, no. 5, pp. 1188–1197, 2012.

	\bibitem{sgbm}
	Heiko Hirschmüller\\
	\textit{ Stereo Processing by Semi-Global Matching and Mutual Information}\\
	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
	 
	\bibitem{rvc}
	Peter Corke\\
	\textit{Robotics, Vision and Control}\\
	Springer, ISBN 978-3-319-54413-7, chapter 11+12, page 319+

	\bibitem{ransac}
	wikipedia\\
	\textit{RANSAC}\\
	https://de.wikipedia.org/wiki/RANSAC-Algorithmus

	\bibitem{bundleadjustment}
	B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon\\
	\textit{Bundle adjustment a modern synthesis}\\
 	in Vision algorithms: theory and practice, 2000, pp. 298–372. 

	\bibitem{Wu}
	Ying Wu,
	\textit{Image Formation and Camera Calibration}
	http://users.eecs.northwestern.edu/~yingwu/teaching/EECS432/Notes/camera.pdf (19.08.2018), Electrical Engineering \& Computer Science Northwestern University Evanston

	\bibitem{pangolin}
	Steven Love Grove\\
	\textit{Pangolin Project}\\
	https://github.com/stevenlovegrove/Pangolin.git

  \bibitem{orbslam2_impl}
  Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos\\
  \textit{ORB-SLAM2 Implemenation}\\
	https://github.com/raulmur/ORB\_SLAM2

  \bibitem{orbslam2_se}
  Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos\\
  \textit{ORB-SLAM2 Implemenation modified version}\\
	https://github.com/eichenberger/ORB\_SLAM2.git

	\bibitem{opencv_se}
	OpenCV\\
	\textit{OpenCV computer vision framework (modified version)}\\
	https://github.com/eichenberger/opencv

	\bibitem{opencv_contrib}
	OpenCV\\
	\textit{OpenCV computer vision framework extensions}\\
	https://github.com/opencv/opencv\_contrib

\end{thebibliography}


\end{document}
