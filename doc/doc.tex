\documentclass[11pt,a4paper,titlepage,oneside]{report}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{float}
\usepackage{subfig}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}

\input{layout}

\title{ORB Slam Point Cloud generation on Apalis iMX8}
\author{Stefan Eichenberger}
\date{December 2018}
\advisors{Marcus Hudritsch}
\department{TSM CPVR Lab}

\begin{document}
\maketitle

\begin{abstract}
	For robotic navigation a map of it's environment is required. Robots often have constraints regarding processor performance. This work analyzes different approaches in regards of their ability to run on an embedded processor using a stereo camera.
\end{abstract}

\section*{Executive Summary}
Simultaneous Location and Mapping (SLAM) is an important topic in computer vision today. The possibilities for this technology are infinite. SLAM can be used for navigation, object recognition and augmented reality. Often this algorithms need a high performance graphics card or a high performance processor. For industrial and robotics purposes space, thermal and power constraints don't allow the usage of a high performance CPUs. This work analyzes one specific SLAM algorithm on how it performs on a modern embedded ARM CPU. The final goal is to have a system available that generates a dense point cloud that can be later used to extract a map of the environment.\\\\
As embedded CPU we use an iMX8 Quad Max from NXP with hardware provided by Toradex. This processor is an early silicon and not available publicly yet. It is one of the fastest embedded processor available for industrial purposes available today.\\

\tableofcontents

\chapter{Introduction}
For robot navigation we have several possibilities today. Navigation systems can use IMUs, magneto meters, radio signals and for outdoor navigation GPS. Humans do most navigation with their eyes. Today we have more computational power available and therefore computer vision becomes more and more important. One growing field in computer vision is SLAM where we try to create a map of the environment and estimate the pose of the camera based on images.\\\\
In this project we analyze the performance of ORB SLAM \cite{orbslam} running on an iMX8QM. We use a Stereo Camera for creating a point cloud. By using a Stereo Camera we are able to reconstruct a point cloud with a known scale.\\\\
After porting ORB SLAM to the iMX8 we analyze the performance issues and start a discussion about possible alternatives.

\section{Apalis iMX8QM}

Toradex is interested to see some new applications and demos running on their new Apalis iMX8QM. Therefore, we concluded to try to port a computer vision application like SLAM to this platform. One area where NXP tries to place the iMX8 is computer vision in industry and automation, that's why it should be a good fit for this type of application. NXP has launched different types of iMX8. Today available is only the iMX8M which is performance wise placed between the iMX8 and the iMX8X. The Apalis iMX8QM features the most performant variant of the iMX8.

\subsection{Features}

The iMX8QM contains the following features:
\begin{itemize}
  \item Two Cortex A72 high performance processors
  \item Four Cortex A53 low power processors
  \item Two Cotex M4 realtime processors 
  \item Two Vivante GC7000 GPUs for graphics and GPGPU
  \item Industrial temperature range from -40°C to 85°C
\end{itemize}

\section{Goals}
The goal of this project work are the followings:
\begin{itemize}
\item Porting ORB SLAM to Apalis iMX8
\item Analyzing different possibilities to speed up the algorithm
\item Adding an extension to create a dense cloud
\item Starting discussion about possible alternatives to ORB SLAM
\end{itemize}

\section{Time Plan}

The project was planned to end on beginning of January. The concept phase started well. Unfortunately, standard ORB SLAM doesn't perform very well in the standard implementation. First optimizations only improved the performance a little bit. However, we decided then to do a dense cloud generation first, to show a possible implementation. The time plan of this project is shown in figure \ref{fig:timeplan}.
\begin{figure}[H]
	\includegraphics[width=1.0\textwidth]{img/timeplan.png}
	\caption{Time Plan}\label{fig:timeplan}
\end{figure}

\chapter{SLAM Evaluation}

As a first task we study some existing SLAM solutions. There are two groups of SLAM systems one are indirect methods and the other one are direct methods.\\\\
On indirect methods an image is analyzed and feature points are extracted. This feature points are then matched against the second stereo image and the depth of the keypoint is calculated. For the next frame again features points on the new left image are extraced and then compared with the 3d Cloud. Through matched keypoints we can then estimated the translation and rotation between the two captures.\\\\
Direct methods on the other hand operated directly on intensity variances. This means the intensities differences between two images is tried to minimize. This can be very computational intensive. Therefore, this methods often operated only on edges or even corners of the image, to reduce the computational effort. Such methods are called direct semi dense or direct sparse methods.\\\\
In the next section we will discuss the differences between monocular SLAM and stereo SLAM. We will see that they are quite similar but that stereo SLAM has some advantages over monocular SLAM.

%TODO: Image with indirect and direct methods

\section{Stereo vs Mono}

In this project the focus lays on Stereo SLAM. A lot of todays paper focus more on monocular SLAM. The reason for that is that 1. Monocular cameras are cheaper and 2. Monocular cameras are available in Smartphones. Monocular SLAM has two main problem. First an initialization process needs to estimated the movement between two frames. A 3D point cloud can only be produced with two images where the camera position was known. In the monocular the position of two images is random therefore it needs some guessing and optimization to find the pose and transformation until a first point cloud can be generated. Event if the initialization succeeded we don't know the moving distance in the real world between the two images. This means we have to guess some scale factor which will be unknown for the whole tracking. This means monocular SLAM finds out movement distances between two poses with regards to it's initialization moving distance but without a known scale. In modern Smartphones this is fixed by using IMU data which gives you the real world movement and therefore the scale factor.\\
For stereo SLAM we don't have this issues. For each camera pose we get two images with known the distance between the two camera sensors (baseline). Therefore, we can theoretically calculate the depth of each pixel in real world distance. From this we can already generate a point cloud assuming that the initial camera pose starts at (0,0,0) world coordinate. We therefore have an instantaneous initialization and can calculate the real world position in e.g. meters.\\
Tracking between two camera poses is however similar for monocular and stereo SLAM. For stereo SLAM it is often only done on the left image. However keypoints can again be inserted immediately on stereo cameras without the need of using a second camera pose. This cam make stereo SLAM more robust against loose of tracking.\\
In the next section we will compare indirect SLAM methods against direct SLAM methods.

\section{Indirect method}

A well documented indirect method is ORB SLAM \cite{orbslam}. Stereo ORB SLAM extracts keypoints on the left and right image. It matches the keypoints on the left and right image and calculates the depth of each point. Based on this points it generates a first point cloud. For the next camera pose it extracts again the keypoints. This keypoints are then matched against the keypoints in the 3d cloud and the translation and rotations between the two poses is calculated.\\\\
Because we know the descriptor of each keypoint we can use triangulation and RANSAC to estimate the pose.\\\\
Indirect methods have the advantage that the pose estimation can be done direct and isn't computational expensive. On the other side the computation of keypoint features is more expensive.

\section{Direct method}

There are two different kinds of direct methods. Dense direct methods and sparse direct methods. The difference is that for Dense direct methods all pixels of the image are used to do the tracking while for sparse methods only a subset of pixels of the image is used.

\subsection{Dense}

There are not that many dense SLAMs. One is DTAM which uses the intensity values of the whole image to do the pose estimation. The idea is to minimize the energy between two images by optimizing the camera pose. The energy defined at a specific position in the image is defined as shown in \ref{eq:pixel_energy}. To estimate the Pose it tries to find a P that minimizes $E_{t}$ as shown in \ref{eq:total_energy}.
\begin{equation}\label{eq:pixel_energy}
  E_{u}=I_1(u)-I_2(P*U)
\end{equation}
Where:
\begin{align*}
  E_{u}		&: \text{Energy at a specific position}\\
  I_{1/2}:	&: \text{Image 1/2}\\
  u:		&: \text{Position in image} \\
  P:		&: \text{Pose of the camera} \\
  U:		&: \text{Position of x in 3D}
\end{align*}
\begin{equation}\label{eq:total_energy}
  E_{t}=\min(\sum_{x=0}^X\sum_{y=0}^YE_u)
\end{equation}

One of the biggest advantages of using a direct dense method is that we receive a dense point cloud which can be used directly to create maps and 3d objects. A big disadvantage is however that they are quite computational expensive. Therefore they are unfortunately not appropriate for embedded devices.

\subsection{Sparse}

% TODO: Add references to DSO, FAST and LSD
Sparse direct SLAMs like SVO don't optimize the Energy over the whole image but use some sparse points instead. One possibility of a sparse point is e.g. a corner point found by FAST. Similar to sparse methods there are semi dense methods which try to minimize the energy with several points laying on edges. They can e.g. be found by Canny Edge detection or DoG. The advantage of sparse and semi dense are that such methods are less computational expensive than dense methods.\\\\

Sparse direct methods don't create dense clouds immediately however they are able to work on weaker keypoints than indirect methods. Therefore they can create denser clouds than e.g. ORB SLAM. They can also be computational less expensive than indirect methods because they don't have to calculate expensive features.

\section{Decision}

We decided to give ORB SLAM a first try because of the open source availability of this algorithm. Also there is some experience at the CPVR lab with this algorithm and 8-12 fps were achieved on a modern smartphone. The iMX8 has a processor with similar power as todays mid end smartphone therefor the hope is to achieve similar performances.

\chapter{Camera Evaluation}

To do stereo vision we of course need a stereo camera. There are a few stereo camera available on the market and it would also be possible to build a stereo camera on our own.

\section{ZED}
The ZED camera is an extremely promising camera for stereo vision. Here a list of the features:
\begin{itemize}
	\item Full HD color images with 30fps
	\item USB3.0
	\item UVC compliant
	\item Linux SDK
	\item Baseline 120mm
	\item IMU sensor with 6DoF
	\item Price: 449\$
\end{itemize}

To run their SDK a Dual Core CPU with 2.3 GHz and CUDA > 3.0 is required. However, it's also possible to use the camera without their SDK.

\section{ECON Tara}
The ECON Tara is another stereo camera. In comparison to the ZED camera it delivers reduced resolution and only gray scale images.
\begin{itemize}
	\item 752x480 gray scale images with 60fps
	\item USB3.0
	\item Global Shutter Camera
	\item UVC compliant
	\item Baseline 60mm
	\item IMU sensor with 6DoF
	\item Open Source SDK
	\item Price: 149\$
\end{itemize}

\section{Intel RealSense D430}
The Intel RealSense camera is a infrared stereo camera. In comparison to RGB stereo camera it uses infrared images for stereo vision. Additionally it also has an RGB sensor.
\begin{itemize}
	\item 1280x720 with 90fps (IR), 1920x1080 with 30fps (RGB)
	\item USB3.0
	\item Global Shutter Camera
	\item UVC compilant (kernel patch required)
	\item Baseline 50mm
	\item Open Source SDK (librealsense)
	\item Price: 179\$
\end{itemize}

\section{Decision}

Because of the good price ration and because it is a conventional stereo camera, we decide to use the ECON Tara stereo camera. However, the Intel RealSense camera would be a great fit as well because it delivers a depth image directly. This would reduce the CPU load because the camera does the depth calculation. If Full HD is required the ZED would be the perfect fit, however keep in mind that for embedded systems Full HD processing may not be possible.

\chapter{Camera Calibration}

Camera calibration is necessary to find a model that expresses the properties of a camera. Properties of a camera are:

\begin{itemize}
	\item Focal length of the camera lense
	\item Principal point on the camera sensor, where the z axis of the cameras coordinate system goes through.
	\item Distortion of the camera lense
	\item Size of a pixel
\end{itemize}

If we once know the camera model and the camera position, we can project virtual objects into a real world image. Figure \ref{fig:model} shows an image of a checkerboard where we project a virtual cube onto. One camera model in this image took distortion into account while the other one ignored it.
\begin{figure}[H]
  \begin{center}
		\includegraphics[width=1.0\textwidth]{img/model.png}
  \end{center}
	\caption{Applied camera model with (red) and without(green) distortion}\label{fig:model}
\end{figure}

Additional to the above parameters we need to align the images of the left and right camera in stereo vision. This process is called rectifying. The cameras aren't perfectly aligned in horizontal direction however we need to have them aligned because this makes it possible to search for correspondence only on the epipolar line. Further the cameras are slightly rotated which means we must inverse rotate the image to have a perfect alignment. So additional to the properties of the camera we need to know the following parameters:
\begin{itemize}
	\item Rotation matrix of the right camera in regards to the left
	\item Y offset of the right and left camera in pixels
	\item Maximum overlapping region of left and right camera
\end{itemize}


\section{Camera Model}

The camera model expresses how any point in the three-dimensional space is projected onto a two-dimensional image. As a first approximation it assumes that all rays are going trough one point. This is called the pinhole camera model \ref{fig:projection}a. Given this assumption we can describe the projection of a 3D point onto a 2D image as shown in equation \ref{eq:cm}. We calculate the pixel location $x,y$ on the image by normalizing with $s$ as show in equation \ref{eq:cm_normalized} \cite{rvc}.
\begin{equation}\label{eq:cm}
  \begin{pmatrix}
		f_x & \gamma & c_x \\
		0 & f_y & c_y \\
		0 & 0 & 1 \\
	\end{pmatrix}*
	\begin{pmatrix}
		r_{00} & r_{01} & r_{02} & t_x \\
		r_{10} & r_{11} & r_{12} & t_y \\
		r_{20} & r_{21} & r_{22} & t_z \\
	\end{pmatrix}
	\begin{pmatrix}
		X \\
		Y \\
		Z \\
		1
	\end{pmatrix}=
	\begin{pmatrix}
		u \\
		v \\
		s
  \end{pmatrix}
\end{equation}
\begin{equation}\label{eq:cm_normalized}
	\begin{pmatrix}
		x \\
		y
	\end{pmatrix}=
	\begin{pmatrix}
		u/s \\
		v/s 
  \end{pmatrix}
\end{equation}

Where:
\begin{align*}
  X,Y,Z			&: \text{point in the 3D world}\\
	u,v,s	   	&: \text{point in 2D image not normalize}\\
	x,y				&: \text{point in 2D image normalized with s}\\
	f_x,f_y  	&: \text{focal length of the camera}\\
  c_x,c_y  	&: \text{principal point}\\
  t_x,t_y,t_z	&: \text{location of the camera}\\
  r_{ij}	&: \text{part of the rotation matrix}
\end{align*}

We can describe the intuition as follows. A Point (X,Y,Z) is projected onto an image sensor (u/s,v/s) by the multiplication of the intrinsic times the extrinsic matrix. The extrinsic matrix describes where the pinhole of the camera is located in the three dimensional space. The intrinsic camera matrix describes how the camera is constructed. For example, in figure \ref{fig:projection}b we translate and rotate a point $p_i$ with the extrinsic matrix so we can describe its coordinates with the pinhole as origin. Further we transform the point with the intrinsic camera matrix onto the image sensor.\\
\em
Note:\\
In the previous section we said we also need to know the pixel size. This is true but will not appear directly in the camera model. The parameters fx and fy who are sizes in meters are expressed in pixels. They therefore include the information about pixel size.
\normalfont

\begin{figure}[H]
	\centering
	\subfloat[Pinhole model]{\includegraphics[width=0.5\textwidth]{img/pinhole_camera_model.png}}
	\subfloat[3D point to 2D point]{\includegraphics[width=0.5\textwidth]{img/pnp.jpg}}
	\caption{Image projection}\label{fig:projection}
\end{figure}

\section{Stereo Camera Model}

\chapter{ORB SLAM}\label{chap:implementation}

In this chapter we discuss more deeply how ORB SLAM works and how we port it to the iMX8 processor.

\section{How it works}
ORB SLAM tries to find corners with the FAST corner detection. This gives us points which should lay on the intersection between two edges. We call this points keypoints. At the corners we later calculate the ORB descriptors which describes the keypoint.\\
If we have a stereo image we can search for the keypoints on both images and from that calculate the depth of each point. For this we need to do the steps shown in figure \ref{fig:orb_slam2}.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=0.9\textwidth]{img/orb_slam2.png}
  \end{center}
	\caption{ORB SLAM2 \cite{orbslam2}}\label{fig:orb_slam2}
\end{figure}

ORB SLAM uses multiple threads to improve the performance of the algorithm. \\
Tracking:
\begin{enumerate}
	\item Match descriptors from the left and the right image
	\item Calculate the difference in pixels between the keypoint on the left and on the right image
	\item Calculate the depth of the keypoint with formula shown in equation \ref{eq:depth}
	\item With known depth calculate the point position with camera as reference
	\item Match point descriptors with point descriptors from keyframe
	\item Estimate new camera pose with triangulation and RANSAC
	\item Check if this frame is a potential keyframe.
\end{enumerate}
The potential keyframe check is done by checking how many frames were captured since the last keyframe, if a lot of outliers were detected and if more than 35\% of points are new with regards to the reference keyframe.\\

Local Mapping:
\begin{enumerate}
	\item Transform the new keypoints to the global coordinate system based on the camera pose
	\item Do a bundle adjustment. It is likely that points we already know do not 100\% match the position where we see the points. Bundle adjustment fuses the position of points seen more than once.
	\item Check if more than 10\% of the points in the current frame compared to the points in all keyframes are new. If not we drop the keyframe for future use.
	\item 
\end{enumerate}

Loop Closing:
\begin{enumerate}
	\item Match latest keyframe with all keyframes stored in the database with DBoW2 \cite{dbow}
	\item Optimize correspondent point by doing bundle adjustment over key frame poses and point location
\end{enumerate}

\begin{equation}\label{eq:depth}
	d=\frac{b}{\Delta}\\
	\Delta=u_{left}-u_{right}
\end{equation}
\begin{align*}
	d &:					\text{depth}\\
	b &:					\text{baseline (distance between left and right camera)}\\
	\Delta &:			\text{Difference between x position on left and rigth image}\\
	\u_{left} &:	\text{X Position of keypoint on left image}\\
	\u_{right} &: \text{X Position of keypoint on right image}
\end{align*}

\begin{equation}\label{eq:orb_pos}
	y=\frac{d*u}{f_u}\\
	x=\frac{d*v}{f_v}
\end{equation}
\begin{align*}
	y &:				\text{Y position with camera 0}\\
	x &: 				\text{X position with camera 0}\\
	d &: 				\text{Depth=Z position with camera 0}\\
	u,v &:			\text{X,Y position on image}\\
	f_u,f_v &:	\text{Focal length}
\end{align*}

\chapter{Densification}

\chapter{Reflection}

\chapter{Direct Approach}

\chapter{Conclusion}

\listoffigures
 
\begin{thebibliography}{1}

  \bibitem{orbslam}
  Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos
  \textit{ORB-SLAM: a Versatile and Accurate Monocular SLAM System}
  arXiv:1502.00956v2

  \bibitem{orbslam2}
  Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos
  \textit{ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras}
	arXiv:1610.06475v2 

  \bibitem{orbslam2_impl}
  Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos
  \textit{ORB-SLAM2 Implemenation}
	https://github.com/raulmur/ORB\_SLAM2

	\bibitem{dbow}
	D. Gálvez-López and J. D. Tardós,
	\textit{Bags of binary words for fast place recognition in image sequences}
	IEEE Trans. Robot., vol. 28, no. 5, pp. 1188–1197, 2012.


  \bibitem{selfcalib}
  Daniel Herrera et al.
  \textit{Forget the checkerboard: practical self-calibration using a planar scene}
  doi:10.1109/WACV.2016.7477641

  \bibitem{Hayes}
  Monson H. Hayes,
  \textit{Statistical Digital Signal Processing And Modeling},
  Wiley, ISBN 0-47159431-8

  \bibitem{pinv}
  Numpy,
  \textit{scipy.linalg.pinv},
  Compute the (Moore-Penrose) pseudo-inverse of a matrix

  \bibitem{gauss_newton}
  Wikipedia,
  \textit{Gauss-Newton algorithm},
  https://en.wikipedia.org/wiki/Gauss\%E2\%80\%93Newton\_algorithm (08.07.2018)

  \bibitem{newton_image}
  http://fourier.eng.hmc.edu,
  \textit{Newton Ralphson Method}
  http://fourier.eng.hmc.edu/e176/lectures/NM/node20.html (09.07.2018)

  \bibitem{Zhang}
  Zhengyou Zhang,
  \textit{A Flexible New Technique for Camera Calibration}
  MSR-TR-98-71

	\bibitem{rvc}
	Peter Corke,
	\textit{Robotics, Vision and Control}
	Springer, ISBN 978-3-319-54413-7, chapter 11+12, page 319+

	\bibitem{qr_decomposition}
	William H. Press,
	\textit{Numerical Recipes 3rd Edition: The Art of Scientific Computing, p102-109} 
	Cambridge University Press, ISBN 0-52188068-8

	\bibitem{rq_stack}
	johnnycrab,
	\textit{rq-decomposition} 
	https://math.stackexchange.com/questions/1640695/rq-decomposition (10.07.2018)

	\bibitem{ransac}
	wikipedia,
	\textit{RANSAC}
	https://de.wikipedia.org/wiki/RANSAC-Algorithmus

	\bibitem{BFMatcher}
	OpenCV,
	\textit{Brute-force descriptor matcher}
	https://docs.opencv.org/3.1.0/d3/da1/classcv\_1\_1BFMatcher.html

	\bibitem{Jacobian}
	Wikipedia,
	\textit{Jacobian matrix}
	https://en.wikipedia.org/wiki/Jacobian\_matrix\_and\_determinant

	\bibitem{Wu}
	Ying Wu,
	\textit{Image Formation and Camera Calibration}
	http://users.eecs.northwestern.edu/~yingwu/teaching/EECS432/Notes/camera.pdf (19.08.2018), Electrical Engineering \& Computer Science Northwestern University Evanston

\end{thebibliography}


\end{document}
